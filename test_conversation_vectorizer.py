"""
Test script for ConversationVectorizer
"""

import os
import sys

# Add src directory to Python path
sys.path.append(os.path.join(os.path.dirname(__file__), "src"))

from dotenv import load_dotenv
from core.conversation_vectorizer import ConversationVectorizer


def main():
    """Test ConversationVectorizer functionality"""
    load_dotenv()

    print("ğŸ§ª Testing ConversationVectorizer...")

    # Get authentication info from environment variables
    zilliz_uri = os.getenv("ZILLIZ_URI")
    zilliz_token = os.getenv("ZILLIZ_TOKEN")

    if not zilliz_uri or not zilliz_token:
        print("âŒ ZILLIZ_URI or ZILLIZ_TOKEN not found in environment")
        return

    try:
        # Initialize vectorizer
        print("ğŸ”§ Initializing ConversationVectorizer...")
        vectorizer = ConversationVectorizer(
            zilliz_uri,
            zilliz_token,
            chunk_size=200,
            chunk_overlap=40,
        )

        # Test with sample data
        sample_texts = [
            "ã“ã‚Œã¯æ—¥æœ¬èªã®ãƒ†ã‚¹ãƒˆãƒ†ã‚­ã‚¹ãƒˆã§ã™ã€‚è‡ªç„¶è¨€èªå‡¦ç†ã«ã¤ã„ã¦å­¦ç¿’ã—ã¦ã„ã¾ã™ã€‚",
            "æ©Ÿæ¢°å­¦ç¿’ã¨ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã®çµ„ã¿åˆã‚ã›ã¯éå¸¸ã«å¼·åŠ›ã§ã™ã€‚ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æ¤œç´¢ã‚’å®Ÿè£…ã—ã¾ã™ã€‚",
            "å¯¾è©±å‹AIã‚·ã‚¹ãƒ†ãƒ ã®é–‹ç™ºã«ã¤ã„ã¦èª¬æ˜ã—ã¾ã™ã€‚ãƒ¦ãƒ¼ã‚¶ãƒ¼ã¨ã®è‡ªç„¶ãªä¼šè©±ã‚’ç›®æŒ‡ã—ã¾ã™ã€‚",
        ]

        print("ğŸ“ Processing sample texts...")
        for i, text in enumerate(sample_texts):
            chunks = vectorizer.process_monologue(text, f"sample_{i+1}.txt")
            print(f"âœ… Processed sample {i+1}: {len(chunks)} chunks created")

        # Test searches
        print("\nğŸ” Testing hybrid search...")
        try:
            hybrid_results = vectorizer.hybrid_search("æ©Ÿæ¢°å­¦ç¿’", limit=3)
            print(f"âœ… Hybrid search returned {len(hybrid_results)} results")
            for i, result in enumerate(hybrid_results, 1):
                print(f"  {i}. {result.text[:60]}... (Score: {result.score:.3f})")
        except Exception as e:
            print(f"âŒ Hybrid search failed: {e}")

        print("\nğŸ” Testing dense search...")
        try:
            dense_results = vectorizer.search_similar("è‡ªç„¶è¨€èªå‡¦ç†", limit=3)
            print(f"âœ… Dense search returned {len(dense_results)} results")
            for i, result in enumerate(dense_results, 1):
                print(f"  {i}. {result.text[:60]}... (Score: {result.score:.3f})")
        except Exception as e:
            print(f"âŒ Dense search failed: {e}")

        # Show stats
        print("\nğŸ“Š Vectorizer Stats:")
        try:
            stats = vectorizer.get_stats()
            print(
                f"Collection entities: {stats['zilliz_stats'].get('num_entities', 'Unknown')}"
            )
            print(
                f"Tokenizer available: {stats['text_processor']['tokenizer_available']}"
            )
            print(
                f"Sparse vectorizer fitted: {stats['vector_generator']['sparse_fitted']}"
            )
        except Exception as e:
            print(f"âš ï¸ Could not get stats: {e}")

        print("\nğŸ‰ ConversationVectorizer test completed!")

    except Exception as e:
        print(f"âŒ An error occurred: {e}")
        import traceback

        traceback.print_exc()


if __name__ == "__main__":
    main()
