import os
import json
from datetime import datetime
from typing import List, Dict, Any
from dataclasses import dataclass

# å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒª
from langchain.text_splitter import RecursiveCharacterTextSplitter
from sentence_transformers import SentenceTransformer
from pymilvus import connections, Collection, FieldSchema, CollectionSchema, DataType
import numpy as np

from dotenv import load_dotenv

load_dotenv()


@dataclass
class ConversationChunk:
    """ä¼šè©±ãƒãƒ£ãƒ³ã‚¯ã®ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒ©ã‚¹"""

    id: str
    text: str
    speaker: str
    timestamp: str
    chunk_index: int
    original_length: int


class ConversationVectorizer:

    def __init__(
        self,
        zilliz_uri: str,
        zilliz_token: str,
        embedding_model: str = "sonoisa/sentence-bert-base-ja-mean-tokens-v2",
    ):
        """
        åˆæœŸåŒ–
        Args:
            zilliz_uri: Zilliz Cloudã®URI
            zilliz_token: Zilliz Cloudã®ãƒˆãƒ¼ã‚¯ãƒ³
            embedding_model: ä½¿ç”¨ã™ã‚‹åŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ«
        """
        self.zilliz_uri = zilliz_uri
        self.zilliz_token = zilliz_token
        self.embedding_model = SentenceTransformer(embedding_model)
        self.collection_name = "conversation_chunks"

        # ãƒ†ã‚­ã‚¹ãƒˆåˆ†å‰²å™¨ã®åˆæœŸåŒ–
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=300,  # ãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚º
            chunk_overlap=50,  # ã‚ªãƒ¼ãƒãƒ¼ãƒ©ãƒƒãƒ—
            separators=["\n\n", "\n", "ã€‚", "ï¼", "ï¼Ÿ", " ", ""],
        )

        self._connect_to_zilliz()
        self._setup_collection()

    def _connect_to_zilliz(self):
        """Zilliz Cloudã«æ¥ç¶š"""
        try:
            connections.connect(
                alias="default", uri=self.zilliz_uri, token=self.zilliz_token
            )
            print("âœ… Zilliz Cloudã«æ¥ç¶šã—ã¾ã—ãŸ")
        except Exception as e:
            print(f"âŒ Zilliz Cloudæ¥ç¶šã‚¨ãƒ©ãƒ¼: {e}")
            raise

    def _setup_collection(self):
        """ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ã®è¨­å®š"""
        # ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã‚¹ã‚­ãƒ¼ãƒã®å®šç¾©
        fields = [
            FieldSchema(
                name="id", dtype=DataType.VARCHAR, max_length=100, is_primary=True
            ),
            FieldSchema(
                name="embedding", dtype=DataType.FLOAT_VECTOR, dim=768
            ),  # all-MiniLM-L6-v2ã®æ¬¡å…ƒæ•°
            FieldSchema(name="text", dtype=DataType.VARCHAR, max_length=2000),
            FieldSchema(name="speaker", dtype=DataType.VARCHAR, max_length=100),
            FieldSchema(name="timestamp", dtype=DataType.VARCHAR, max_length=50),
            FieldSchema(name="chunk_index", dtype=DataType.INT64),
            FieldSchema(name="original_length", dtype=DataType.INT64),
        ]

        schema = CollectionSchema(fields, "ä¼šè©±ãƒãƒ£ãƒ³ã‚¯ã®ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³")

        # ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ã®ä½œæˆï¼ˆæ—¢å­˜ã®å ´åˆã¯å‰Šé™¤ï¼‰
        try:
            from pymilvus import utility

            if utility.has_collection(self.collection_name):
                utility.drop_collection(self.collection_name)

            self.collection = Collection(self.collection_name, schema)
            print(f"âœ… ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ '{self.collection_name}' ã‚’ä½œæˆã—ã¾ã—ãŸ")
        except Exception as e:
            print(f"âŒ ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ä½œæˆã‚¨ãƒ©ãƒ¼: {e}")
            raise

    def parse_monologue(self, text: str) -> List[Dict[str, Any]]:
        """
        ä¸€äººèªã‚Šã®ãƒ†ã‚­ã‚¹ãƒˆã‚’æ„å‘³ã®ã‚ã‚‹å˜ä½ã«åˆ†æ
        Args:
            text: ä¸€äººèªã‚Šã®ãƒ†ã‚­ã‚¹ãƒˆ
        Returns:
            åˆ†å‰²ã•ã‚ŒãŸå†…å®¹ã®ãƒªã‚¹ãƒˆ
        """
        utterances = []

        # æ”¹è¡Œã§åŒºåˆ‡ã‚‰ã‚Œã¦ã„ã‚‹å ´åˆã¯æ®µè½å˜ä½ã§å‡¦ç†
        if "\n" in text:
            paragraphs = [p.strip() for p in text.split("\n") if p.strip()]
            for i, paragraph in enumerate(paragraphs):
                utterances.append(
                    {
                        "speaker": "Speaker",
                        "content": paragraph,
                        "timestamp": datetime.now().isoformat(),
                        "paragraph_index": i,
                    }
                )
        else:
            # æ”¹è¡ŒãŒãªã„å ´åˆã¯å¥ç‚¹ã§åˆ†å‰²
            sentences = []
            current_sentence = ""

            for char in text:
                current_sentence += char
                if char in ["ã€‚", "ï¼", "ï¼Ÿ"]:
                    if current_sentence.strip():
                        sentences.append(current_sentence.strip())
                        current_sentence = ""

            # æœ€å¾Œã®æ–‡ãŒå¥ç‚¹ã§çµ‚ã‚ã‚‰ãªã„å ´åˆ
            if current_sentence.strip():
                sentences.append(current_sentence.strip())

            for i, sentence in enumerate(sentences):
                utterances.append(
                    {
                        "speaker": "Speaker",
                        "content": sentence,
                        "timestamp": datetime.now().isoformat(),
                        "sentence_index": i,
                    }
                )

        return utterances

    def chunk_conversations(
        self, utterances: List[Dict[str, Any]]
    ) -> List[ConversationChunk]:
        """
        ç™ºè©±ã‚’ãƒãƒ£ãƒ³ã‚¯ã«åˆ†å‰²
        Args:
            utterances: ç™ºè©±ã®ãƒªã‚¹ãƒˆ
        Returns:
            ãƒãƒ£ãƒ³ã‚¯ã®ãƒªã‚¹ãƒˆ
        """
        chunks = []
        chunk_id = 0

        for utterance in utterances:
            content = utterance["content"]

            # é•·ã„ç™ºè©±ã¯åˆ†å‰²
            if len(content) > 300:
                text_chunks = self.text_splitter.split_text(content)
                for i, chunk_text in enumerate(text_chunks):
                    chunks.append(
                        ConversationChunk(
                            id=f"chunk_{chunk_id:06d}",
                            text=chunk_text,
                            speaker=utterance["speaker"],
                            timestamp=utterance["timestamp"],
                            chunk_index=i,
                            original_length=len(content),
                        )
                    )
                    chunk_id += 1
            else:
                # çŸ­ã„ç™ºè©±ã¯ãã®ã¾ã¾
                chunks.append(
                    ConversationChunk(
                        id=f"chunk_{chunk_id:06d}",
                        text=content,
                        speaker=utterance["speaker"],
                        timestamp=utterance["timestamp"],
                        chunk_index=0,
                        original_length=len(content),
                    )
                )
                chunk_id += 1

        return chunks

    def generate_embeddings(self, chunks: List[ConversationChunk]) -> List[np.ndarray]:
        """
        ãƒãƒ£ãƒ³ã‚¯ã®ãƒ™ã‚¯ãƒˆãƒ«åŒ–
        Args:
            chunks: ãƒãƒ£ãƒ³ã‚¯ã®ãƒªã‚¹ãƒˆ
        Returns:
            åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«ã®ãƒªã‚¹ãƒˆ
        """
        texts = [chunk.text for chunk in chunks]
        embeddings = self.embedding_model.encode(texts)
        print(f"âœ… {len(chunks)}å€‹ã®ãƒãƒ£ãƒ³ã‚¯ã‚’ãƒ™ã‚¯ãƒˆãƒ«åŒ–ã—ã¾ã—ãŸ")
        return embeddings

    def insert_to_zilliz(
        self, chunks: List[ConversationChunk], embeddings: List[np.ndarray]
    ):
        """
        Zilliz Cloudã«ãƒ‡ãƒ¼ã‚¿ã‚’æŒ¿å…¥
        Args:
            chunks: ãƒãƒ£ãƒ³ã‚¯ã®ãƒªã‚¹ãƒˆ
            embeddings: åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«ã®ãƒªã‚¹ãƒˆ
        """
        data = [
            [chunk.id for chunk in chunks],
            embeddings.tolist(),
            [chunk.text for chunk in chunks],
            [chunk.speaker for chunk in chunks],
            [chunk.timestamp for chunk in chunks],
            [chunk.chunk_index for chunk in chunks],
            [chunk.original_length for chunk in chunks],
        ]

        try:
            self.collection.insert(data)
            print(f"âœ… {len(chunks)}å€‹ã®ãƒãƒ£ãƒ³ã‚¯ã‚’Zilliz Cloudã«ä¿å­˜ã—ã¾ã—ãŸ")

            # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®ä½œæˆ
            index_params = {
                "metric_type": "IP",  # Inner Product
                "index_type": "IVF_FLAT",
                "params": {"nlist": 128},
            }
            self.collection.create_index("embedding", index_params)
            self.collection.load()
            print("âœ… ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’ä½œæˆã—ã€ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ã‚’ãƒ­ãƒ¼ãƒ‰ã—ã¾ã—ãŸ")

        except Exception as e:
            print(f"âŒ ãƒ‡ãƒ¼ã‚¿æŒ¿å…¥ã‚¨ãƒ©ãƒ¼: {e}")
            raise

    def process_monologue(self, text: str):
        """
        ä¸€äººèªã‚Šãƒ†ã‚­ã‚¹ãƒˆã®å…¨ä½“å‡¦ç†
        Args:
            text: ä¸€äººèªã‚Šã®ãƒ†ã‚­ã‚¹ãƒˆ
        """
        print("ğŸ”„ ä¸€äººèªã‚Šãƒ†ã‚­ã‚¹ãƒˆã®å‡¦ç†ã‚’é–‹å§‹...")

        # 1. ãƒ†ã‚­ã‚¹ãƒˆã®è§£æ
        utterances = self.parse_monologue(text)
        print(f"ğŸ“ {len(utterances)}å€‹ã®å˜ä½ã«åˆ†å‰²ã—ã¾ã—ãŸ")

        # 2. ãƒãƒ£ãƒ³ã‚¯åŒ–
        chunks = self.chunk_conversations(utterances)
        print(f"âœ‚ï¸ {len(chunks)}å€‹ã®ãƒãƒ£ãƒ³ã‚¯ã«åˆ†å‰²ã—ã¾ã—ãŸ")

        # 3. ãƒ™ã‚¯ãƒˆãƒ«åŒ–
        embeddings = self.generate_embeddings(chunks)

        # 4. Zilliz Cloudã«ä¿å­˜
        self.insert_to_zilliz(chunks, embeddings)

        print("ğŸ‰ å‡¦ç†ãŒå®Œäº†ã—ã¾ã—ãŸï¼")
        return chunks

    def search_similar(self, query: str, limit: int = 5) -> List[Dict]:
        """
        é¡ä¼¼æ¤œç´¢
        Args:
            query: æ¤œç´¢ã‚¯ã‚¨ãƒª
            limit: å–å¾—ä»¶æ•°
        Returns:
            æ¤œç´¢çµæœ
        """
        # ã‚¯ã‚¨ãƒªã®ãƒ™ã‚¯ãƒˆãƒ«åŒ–
        query_embedding = self.embedding_model.encode([query])

        search_params = {"metric_type": "IP", "params": {"nprobe": 10}}

        results = self.collection.search(
            query_embedding,
            "embedding",
            search_params,
            limit=limit,
            output_fields=["text", "speaker", "timestamp"],
        )

        return [
            {
                "text": hit.entity.get("text"),
                "speaker": hit.entity.get("speaker"),
                "timestamp": hit.entity.get("timestamp"),
                "score": hit.score,
            }
            for hit in results[0]
        ]


# ä½¿ç”¨ä¾‹
def main():
    # ã‚µãƒ³ãƒ—ãƒ«ä¸€äººèªã‚Šãƒ†ã‚­ã‚¹ãƒˆ
    sample_monologue = """
ä»Šæ—¥ã¯äººå·¥çŸ¥èƒ½ã®ç™ºå±•ã«ã¤ã„ã¦è€ƒãˆã¦ã¿ãŸã„ã¨æ€ã„ã¾ã™ã€‚
è¿‘å¹´ã€æ©Ÿæ¢°å­¦ç¿’æŠ€è¡“ã®é€²æ­©ã«ã‚ˆã‚Šã€æ§˜ã€…ãªåˆ†é‡ã§ AI ã®æ´»ç”¨ãŒé€²ã‚“ã§ã„ã¾ã™ã€‚ç‰¹ã«è‡ªç„¶è¨€èªå‡¦ç†ã®åˆ†é‡ã§ã¯ã€å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ã®ç™»å ´ã«ã‚ˆã‚Šã€äººé–“ã¨ã»ã¼åŒç­‰ã®æ–‡ç« ç”ŸæˆãŒå¯èƒ½ã«ãªã‚Šã¾ã—ãŸã€‚

ã—ã‹ã—ã€ã“ã‚Œã‚‰ã®æŠ€è¡“ã«ã¯ã¾ã èª²é¡Œã‚‚å¤šãå­˜åœ¨ã—ã¾ã™ã€‚ãƒ‡ãƒ¼ã‚¿ã®å“è³ªã‚„ãƒã‚¤ã‚¢ã‚¹ã®å•é¡Œã€è¨ˆç®—è³‡æºã®å¤§é‡æ¶ˆè²»ã€ãã—ã¦ä½•ã‚ˆã‚Šäººé–“ã®é›‡ç”¨ã¸ã®å½±éŸ¿ãªã©ãŒæ‡¸å¿µã•ã‚Œã¦ã„ã¾ã™ã€‚

ä¸€æ–¹ã§ã€AIæŠ€è¡“ã¯åŒ»ç™‚è¨ºæ–­ã®ç²¾åº¦å‘ä¸Šã‚„ã€æ–°è–¬é–‹ç™ºã®åŠ é€Ÿã€æ°—å€™å¤‰å‹•å¯¾ç­–ãªã©ã€äººé¡ã®é‡è¦ãªèª²é¡Œè§£æ±ºã«ã‚‚å¤§ããè²¢çŒ®ã™ã‚‹å¯èƒ½æ€§ã‚’ç§˜ã‚ã¦ã„ã¾ã™ã€‚é‡è¦ãªã®ã¯ã€æŠ€è¡“ã®ç™ºå±•ã¨äººé–“ç¤¾ä¼šã®èª¿å’Œã‚’ä¿ã¡ãªãŒã‚‰ã€æŒç¶šå¯èƒ½ãªæ–¹æ³•ã§ AI ã‚’æ´»ç”¨ã™ã‚‹ã“ã¨ã ã¨è€ƒãˆã¦ã„ã¾ã™ã€‚
"""

    # ç’°å¢ƒå¤‰æ•°ã‹ã‚‰èªè¨¼æƒ…å ±ã‚’å–å¾—ï¼ˆå®Ÿéš›ã®ä½¿ç”¨æ™‚ï¼‰
    zilliz_uri = os.getenv("ZILLIZ_URI", "your-zilliz-uri")
    zilliz_token = os.getenv("ZILLIZ_TOKEN", "your-zilliz-token")

    try:
        # ãƒ™ã‚¯ãƒˆãƒ©ã‚¤ã‚¶ãƒ¼ã®åˆæœŸåŒ–
        vectorizer = ConversationVectorizer(zilliz_uri, zilliz_token)

        # ä¸€äººèªã‚Šãƒ†ã‚­ã‚¹ãƒˆã®å‡¦ç†
        chunks = vectorizer.process_monologue(sample_monologue)

        # ã‚µãƒ³ãƒ—ãƒ«æ¤œç´¢
        print("\nğŸ” æ¤œç´¢ãƒ†ã‚¹ãƒˆ:")
        results = vectorizer.search_similar("AIã®èª²é¡Œ", limit=3)
        for i, result in enumerate(results, 1):
            print(f"{i}. {result['text'][:100]}... (ã‚¹ã‚³ã‚¢: {result['score']:.3f})")

    except Exception as e:
        print(f"âŒ ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")


if __name__ == "__main__":
    main()
